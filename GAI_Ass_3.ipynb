{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOYfSBkTyJyr2aC6nE46mu1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/2303A52242/GAI_2242/blob/main/GAI_Ass_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write Python code without using any libraries to find the value of x at which the\n",
        "function f(x) shown in equation (1) has minimum value. Consider Gradient Descent Algorithm.\n",
        "f (x) = 5x4 + 3x2 + 10"
      ],
      "metadata": {
        "id": "O8iyc1iB_kk1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNjwGPPX-D9p",
        "outputId": "bd6c9958-d7c1-4e80-ae90-cd5cf6fe53b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x that minimizes f(x) is: 4.810419162406747e-28\n"
          ]
        }
      ],
      "source": [
        "# Gradient Descent to find the minimum of f(x) = 5x^4 + 3x^2 + 10\n",
        "\n",
        "def f(x):\n",
        "    return 5 * x*4 + 3 * x*2 + 10\n",
        "\n",
        "def df(x):\n",
        "    # Derivative of f(x) with respect to x: df/dx = 20x^3 + 6x\n",
        "    return 20 * x**3 + 6 * x\n",
        "\n",
        "def gradient_descent(learning_rate, iterations, initial_x):\n",
        "    x = initial_x\n",
        "    for _ in range(iterations):\n",
        "        gradient = df(x)\n",
        "        x -= learning_rate * gradient\n",
        "    return x\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "initial_x = 0.5\n",
        "\n",
        "# Find the value of x that minimizes f(x)\n",
        "min_x = gradient_descent(learning_rate, iterations, initial_x)\n",
        "print(f\"The value of x that minimizes f(x) is: {min_x}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write Python code without using any libraries to find the value of x and y at which the\n",
        "function g(x,y) shown in equation (2) has minimum value. Consider Gradient Descent Algorithm.\n",
        "f (x) = 3x2 + 5e−y + 10"
      ],
      "metadata": {
        "id": "mOtLyU1j_tcT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent to find the minimum of g(x, y) = 3x^2 + 5e^(-y) + 10\n",
        "\n",
        "def g(x, y):\n",
        "    return 3 * x*2 + 5 * (2.71828*(-y)) + 10\n",
        "\n",
        "def dg_dx(x):\n",
        "    # Partial derivative of g(x, y) with respect to x: dg/dx = 6x\n",
        "    return 6 * x\n",
        "\n",
        "def dg_dy(y):\n",
        "    # Partial derivative of g(x, y) with respect to y: dg/dy = -5e^(-y)\n",
        "    return -5 * (2.71828**(-y))\n",
        "\n",
        "def gradient_descent_2d(learning_rate, iterations, initial_x, initial_y):\n",
        "    x = initial_x\n",
        "    y = initial_y\n",
        "    for _ in range(iterations):\n",
        "        gradient_x = dg_dx(x)\n",
        "        gradient_y = dg_dy(y)\n",
        "        x -= learning_rate * gradient_x\n",
        "        y -= learning_rate * gradient_y\n",
        "    return x, y\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "initial_x = 0.5\n",
        "initial_y = 0.5\n",
        "\n",
        "# Find the values of x and y that minimize g(x, y)\n",
        "min_x, min_y = gradient_descent_2d(learning_rate, iterations, initial_x, initial_y)\n",
        "print(f\"The values of x and y that minimize g(x, y) are: x = {min_x}, y = {min_y}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfjrYXfG-eea",
        "outputId": "bbeee19b-285f-4a52-c455-56e790d2d353"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The values of x and y that minimize g(x, y) are: x = 6.711561962466847e-28, y = 3.946138890954553\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write Python code without using any libraries to find the value of x at which the\n",
        "sigmoid function z(x) shown in equation (3) has minimum value. Consider Gradient Descent\n",
        "Algorithm.\n",
        "z(x) = 1\n",
        "1 + **e−x**"
      ],
      "metadata": {
        "id": "La5Hi_Cc_yzg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Gradient Descent to find the minimum of z(x) = 1 / (1 + e^(-x))\n",
        "\n",
        "def z(x):\n",
        "    return 1 / (1 + 2.71828**(-x))\n",
        "\n",
        "def dz_dx(x):\n",
        "    # Derivative of z(x) with respect to x: dz/dx = z(x) * (1 - z(x))\n",
        "    sig = z(x)\n",
        "    return sig * (1 - sig)\n",
        "\n",
        "def gradient_descent(learning_rate, iterations, initial_x):\n",
        "    x = initial_x\n",
        "    for _ in range(iterations):\n",
        "        gradient = dz_dx(x)\n",
        "        x -= learning_rate * gradient\n",
        "    return x\n",
        "\n",
        "# Parameters\n",
        "learning_rate = 0.01\n",
        "iterations = 1000\n",
        "initial_x = 0.5\n",
        "\n",
        "# Find the value of x that minimizes z(x)\n",
        "min_x = gradient_descent(learning_rate, iterations, initial_x)\n",
        "print(f\"The value of x that minimizes z(x) is: {min_x}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bWOBpxDd-ikS",
        "outputId": "e4117ef5-7a51-4e10-d117-c36d395f982c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The value of x that minimizes z(x) is: -1.6012961162961212\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Write Python code without using any libraries to find the value of optimal values of\n",
        "model parameters M and C such that the model’s Square Error Value shown in equation 4 will\n",
        "be minimum. It means model gives output close to expected output as shown in Figure 1\n",
        "Figura 1: AI Model\n",
        "SE = (ExpectedOutput − P redictedOutput)2 (4)\n",
        "• Expected Leaning Outcomes from this assignment related to python\n",
        "– Students are able to understand gradient descent algorithm to solve both single and\n",
        "multi variable unconstrained non linear optimization problems\n",
        "– Students are able to write code in python for gradient descent algorithm"
      ],
      "metadata": {
        "id": "dmqNHf22_3OY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "# Example data\n",
        "data = [\n",
        "    {'ExpectedOutput': 1, 'Input': 0},\n",
        "    {'ExpectedOutput': 2, 'Input': 1},\n",
        "    {'ExpectedOutput': 3, 'Input': 2},\n",
        "    {'ExpectedOutput': 4, 'Input': 3}\n",
        "]\n",
        "\n",
        "# Model parameters\n",
        "M = random.uniform(-1, 1)  # Initial random value for M\n",
        "C = random.uniform(-1, 1)  # Initial random value for C\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 0.01\n",
        "num_iterations = 1000\n",
        "\n",
        "# Square Error Function\n",
        "def calculate_square_error(data, M, C):\n",
        "    square_error = 0\n",
        "    for point in data:\n",
        "        ExpectedOutput = point['ExpectedOutput']\n",
        "        Input = point['Input']\n",
        "        PredictedOutput = M * Input + C\n",
        "        square_error += (ExpectedOutput - PredictedOutput) ** 2\n",
        "    return square_error\n",
        "\n",
        "# Gradient Descent Algorithm\n",
        "for _ in range(num_iterations):\n",
        "    # Calculate gradients\n",
        "    dM = 0\n",
        "    dC = 0\n",
        "    n = len(data)\n",
        "    for point in data:\n",
        "        ExpectedOutput = point['ExpectedOutput']\n",
        "        Input = point['Input']\n",
        "        PredictedOutput = M * Input + C\n",
        "        dM += -2/n * Input * (ExpectedOutput - PredictedOutput)\n",
        "        dC += -2/n * (ExpectedOutput - PredictedOutput)\n",
        "\n",
        "    # Update parameters\n",
        "    M = M - learning_rate * dM\n",
        "    C = C - learning_rate * dC\n",
        "\n",
        "# Final model parameters\n",
        "print(f\"Optimal M: {M}\")\n",
        "print(f\"Optimal C: {C}\")\n",
        "print(f\"Final Square Error: {calculate_square_error(data, M, C)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJD2hb3v_U1V",
        "outputId": "b7814959-c625-4a03-aa95-42d5c8a03cf6"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal M: 0.9997951400751184\n",
            "Optimal C: 1.0004373844643872\n",
            "Final Square Error: 2.7753634003919666e-07\n"
          ]
        }
      ]
    }
  ]
}